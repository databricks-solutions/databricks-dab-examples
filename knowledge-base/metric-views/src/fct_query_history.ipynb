{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffc94f99-d173-4a36-9b75-d8207477fccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Materialize query history system table for best performance, and so that primary key optimizations can be applied.\n",
    "\n",
    "See [query optimization using primary key constraints](https://docs.databricks.com/aws/en/sql/user/queries/query-optimization-constraints) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65848cfa-7ee4-44e3-91de-c8e18f5ee3be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Catalog & Schema"
    }
   },
   "outputs": [],
   "source": [
    "use catalog identifier(:catalog);\n",
    "use schema identifier(:schema);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b433f0-5869-4e62-afc5-3ae23165b14a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Table"
    }
   },
   "outputs": [],
   "source": [
    "create or replace table fct_query_history(\n",
    "  calendar_key int comment 'FK for calendar dimension (dim_calendar).',\n",
    "  compute_key string comment 'FK for compute dimension (dim_compute).',\n",
    "  workspace_key string comment 'FK for workspace dimension (dim_workspace).',\n",
    "  statement_id string comment 'The ID that uniquely identifies the execution of the statement. You can use this ID to find the statement execution in the Query History UI.',\n",
    "  executed_by string comment 'The email address or username of the user who ran the statement.',\n",
    "  session_id string comment 'The Spark session ID.',\n",
    "  execution_status string comment 'The statement termination state. Possible values are:\n",
    "finished: execution was successful\n",
    "failed: execution failed with the reason for failure described in the accompanying error message\n",
    "canceled: execution was canceled',\n",
    "  executed_by_user_id string comment 'The ID of the user who ran the statement.',\n",
    "  statement_text string comment 'Text of the SQL statement. If you have configured customer-managed keys, statement_text is empty.',\n",
    "  statement_type string comment 'The statement type. For example: alter, copy, and`insert`.',\n",
    "  error_message string comment 'Message describing the error condition. If you have configured customer-managed keys, error_message is empty.',\n",
    "  client_application string comment 'Client application that ran the statement. For example: Databricks SQL, Tableau, and Power BI. Nulls and blanks are labled as Unknown',\n",
    "  client_driver string comment 'The connector used to connect to Databricks to run the statement. For example: Databricks SQL Driver for Go, Databricks ODBC Driver, Databricks JDBC Driver.',\n",
    "  total_duration_ms bigint comment 'Total execution time of the statement in milliseconds ( excluding result fetch time ).',\n",
    "  waiting_for_compute_duration_ms bigint comment 'Time spent waiting for compute resources to be provisioned in milliseconds.',\n",
    "  waiting_at_capacity_duration_ms bigint comment 'Time spent waiting in queue for available compute capacity in milliseconds.',\n",
    "  execution_duration_ms bigint comment 'Time spent executing the statement in milliseconds.',\n",
    "  compilation_duration_ms bigint comment 'Time spent loading metadata and optimizing the statement in milliseconds.',\n",
    "  total_task_duration_ms bigint comment 'The sum of all task durations in milliseconds. This time represents the combined time it took to run the query across all cores of all nodes. It can be significantly longer than the wall-clock duration if multiple tasks are executed in parallel. It can be shorter than the wall-clock duration if tasks wait for available nodes.',\n",
    "  result_fetch_duration_ms bigint comment 'Time spent, in milliseconds, fetching the statement results after the execution finished.',\n",
    "  start_time timestamp comment 'The time when Databricks received the request. Timezone information is recorded at the end of the value with +00:00 representing UTC.',\n",
    "  end_time timestamp comment 'The time the statement execution ended, excluding result fetch time. Timezone information is recorded at the end of the value with +00:00 representing UTC.',\n",
    "  update_time timestamp comment 'The time the statement last received a progress update. Timezone information is recorded at the end of the value with +00:00 representing UTC.',\n",
    "  read_partitions bigint comment 'The number of partitions read after pruning.',\n",
    "  pruned_files bigint comment 'The number of pruned files.',\n",
    "  read_files bigint comment 'The number of files read after pruning.',\n",
    "  read_rows bigint comment 'Total number of rows read by the statement.',\n",
    "  produced_rows bigint comment 'Total number of rows returned by the statement.',\n",
    "  read_bytes bigint comment 'Total size of data read by the statement in bytes.',\n",
    "  read_io_cache_percent tinyint comment 'The percentage of bytes of persistent data read from the IO cache.',\n",
    "  from_result_cache boolean comment 'true indicates that the statement result was fetched from the cache.',\n",
    "  spilled_local_bytes bigint comment 'Size of data, in bytes, temporarily written to disk while executing the statement.',\n",
    "  written_bytes bigint comment 'The size in bytes of persistent data written to cloud object storage.',\n",
    "  shuffle_read_bytes bigint comment 'The total amount of data in bytes sent over the network.',\n",
    "  executed_as_user_id string comment 'The ID of the user or service principal whose privilege was used to run the statement.',\n",
    "  executed_as string comment 'The name of the user or service principal whose privilege was used to run the statement.',\n",
    "  constraint fk_fct_query_history_dim_calendar\n",
    "    foreign key (calendar_key) references dim_calendar (calendar_key),\n",
    "  constraint fk_fct_query_history_dim_compute\n",
    "    foreign key (compute_key) references dim_compute (compute_key),\n",
    "  constraint fk_fct_query_history_dim_workspace\n",
    "    foreign key (workspace_key) references dim_workspace (workspace_key)\n",
    ")\n",
    "cluster by (calendar_key, compute_key, workspace_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55874e3-1f77-448e-9a9e-7721aa561d47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Source Data"
    }
   },
   "outputs": [],
   "source": [
    "insert overwrite fct_query_history\n",
    "select\n",
    "cast(start_time as date) as start_date,\n",
    "year(start_date) * 10000 + month(start_date) * 100 + day(start_date) as calendar_key,\n",
    "case \n",
    "  when compute.type = 'WAREHOUSE' then compute.warehouse_id\n",
    "  when compute.type = 'SERVERLESS_COMPUTE' THEN 'serverless'\n",
    "  else '-1'\n",
    "end as compute_key,\n",
    "workspace_id as workspace_key,\n",
    "statement_id,\n",
    "executed_by,\n",
    "session_id,\n",
    "execution_status,\n",
    "executed_by_user_id,\n",
    "statement_text,\n",
    "statement_type,\n",
    "error_message,\n",
    "case\n",
    "  when client_application is null or client_application in ('', 'unknown') then 'Unknown'\n",
    "  else client_application\n",
    "end as client_application,\n",
    "client_driver,\n",
    "total_duration_ms,\n",
    "waiting_for_compute_duration_ms,\n",
    "waiting_at_capacity_duration_ms,\n",
    "execution_duration_ms,\n",
    "compilation_duration_ms,\n",
    "total_task_duration_ms,\n",
    "result_fetch_duration_ms,\n",
    "start_time,\n",
    "end_time,\n",
    "update_time,\n",
    "read_partitions,\n",
    "pruned_files,\n",
    "read_files,\n",
    "read_rows,\n",
    "produced_rows,\n",
    "read_bytes,\n",
    "read_io_cache_percent,\n",
    "from_result_cache,\n",
    "spilled_local_bytes,\n",
    "written_bytes,\n",
    "shuffle_read_bytes,\n",
    "executed_as_user_id,\n",
    "executed_as\n",
    "from system.query.history\n",
    "where start_time >= current_date() - interval 6 months\n",
    "|> drop start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a18dfb5-f700-487b-8581-54338f42432a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize"
    }
   },
   "outputs": [],
   "source": [
    "optimize fct_query_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531d6c75-087f-432b-b8f6-c648a9a123a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Collect Statistics"
    }
   },
   "outputs": [],
   "source": [
    "analyze table fct_query_history compute statistics for all columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f48149c-7d17-46f0-823b-f0415f1029bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Vacuum"
    }
   },
   "outputs": [],
   "source": [
    "vacuum fct_query_history"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "fct_query_history",
   "widgets": {
    "catalog": {
     "currentValue": "users",
     "nuid": "53aa7b1b-a232-4045-9033-458938e983a9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": "",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "users",
      "label": "",
      "name": "catalog",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "chris_koester",
     "nuid": "ddcd2853-b1ff-4f41-8b46-f3f3eeb9cdf4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "chris_koester",
      "label": "",
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "chris_koester",
      "label": "",
      "name": "schema",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
